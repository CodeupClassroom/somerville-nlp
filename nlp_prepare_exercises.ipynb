{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de65abb4-4125-48dc-9546-3f235a6638e5",
   "metadata": {},
   "source": [
    "# Data Preparation Exercises\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3de94-4358-4f93-b512-c0c2d38bc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unicode, regex, json for text digestion\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "# nltk: natural language toolkit -> tokenization, stopwords\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import acquire\n",
    "from time import strftime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fdb012-c6a7-4c82-a702-c7d3d417d348",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end result of this exercise should be a file named `prepare.py` that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "### 1. Define a function named `basic_clean`. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "* Lowercase everything\n",
    "* Normalize unicode characters\n",
    "* Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f63fc-ce1c-42e6-9423-2cbe7b1ad85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a string and returns the string normalized\n",
    "def basic_clean(string):\n",
    "    # we will normalize our data into standard NFKD unicode, feed it into an ascii encoding\n",
    "    # decode it back into UTF-8\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "    # remove special characters, then lowercase\n",
    "    string = re.sub(r\"[^\\w0-9'\\s]\", '', string).lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd7cf4-e92e-4cd5-a3b5-299c7ec41fc0",
   "metadata": {},
   "source": [
    "### 2. Define a function named `tokenize`. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff11b2fc-53c4-4975-8ce3-9818593981f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions takes in a string and returns a tokenized string\n",
    "def tokenize(string):\n",
    "    # make our tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    # apply our tokenization to the string input\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741c82c-dc6d-4855-a06f-4f80854c4254",
   "metadata": {},
   "source": [
    "### 3. Define a function named `stem`. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10c022-8a26-411b-b08e-e17c9f3119c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a string and returns a string with words stemmed\n",
    "def stem(string):\n",
    "    # create our stemming object\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    # use a list comprehension => stem each word inside of the entire document and split by single spaces\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    # join it together with spaces\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd790d5a-823a-447d-987c-2b8f755a6b36",
   "metadata": {},
   "source": [
    "### 4. Define a function named `lemmatize`. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a32fe4-a83e-4982-89f1-f7e20161577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a string  and returns a string with words lemmatized\n",
    "def lemmatize(string):\n",
    "    # create our lemmatizer object\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    # use a list comprehension to lemmatize each word\n",
    "    # string.split() => output a list of every token inside of the document\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    # join the lemmas together with spaces\n",
    "    string = ' '.join(lemmas)\n",
    "    #return the altered document\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba2bce-cb58-4bff-b19c-2ff0d80da378",
   "metadata": {},
   "source": [
    "### 5. Define a function named `remove_stopwords`. It should accept some text and return the text after removing all the stopwords. \n",
    "\n",
    "### This function should define two optional parameters, `extra_words` and `exclude_words`. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f344cb7-df15-433c-afe7-59ebba5c35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3, 4]\n",
    "list2 = [2, 1, 3, 4]\n",
    "\n",
    "print(set(list1)==set(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 == list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192469b-960f-4261-b255-95b719ac2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = ['a', 'b', 'c', 'c', 'd']\n",
    "\n",
    "myset = set(mylist)\n",
    "\n",
    "print(mylist, myset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be67c1-e9f3-4d25-b319-272a4f20312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a string, optional extra_words and exclued_words parameters with default empty lists and returns a string\n",
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    # use set casting to remove any excluded stopwords\n",
    "    stopword_set = set(stopword_list) - set(exclude_words)\n",
    "    # add in extra words to stopwords set using a union\n",
    "    stopword_set = stopword_set.union(set(extra_words))\n",
    "    # split the document by spaces\n",
    "    words = string.split()\n",
    "    # every word in our document that is not a stopword\n",
    "    filtered_words = [word for word in words if word not in stopword_set]\n",
    "    # join it back together with spaces\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7568c1-5777-4bbc-ad67-59ad21132753",
   "metadata": {},
   "source": [
    "### 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe `news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f6d0b-b30a-4a08-868f-165a20cbfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = acquire.get_news_articles_data()\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e19c2-f45c-4573-8c06-38a42d53c00e",
   "metadata": {},
   "source": [
    "### 7. Make another dataframe for the Codeup blog posts. Name the dataframe `codeup_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd8b3e-3dc2-47d4-b943-2f450f0dce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = acquire.get_blog_articles_data()\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba3d40-9433-4281-9b0a-d6142ec960ee",
   "metadata": {},
   "source": [
    "### 8. For each dataframe, produce the following columns:\n",
    "\n",
    "* `title` to hold the title\n",
    "* `original` to hold the original article/post content\n",
    "* `clean` to hold the normalized and tokenized original with the stopwords removed.\n",
    "* `stemmed` to hold the stemmed version of the cleaned data.\n",
    "* `lemmatized` to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d143d85-0525-4887-9be4-a2edbc8848b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work flow:\n",
    "#  1. clean: normalized/tokenized, with stopwords removed apply: basic_clean, tokenize, remove_stopwords\n",
    "\n",
    "#  2. stemmed: stemmed version of cleaned data apply: stem function onto cleaned data\n",
    "\n",
    "#  3. lemmatized: lemmatized version of cleaned data apply: lemmatize function onto cleaned datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f9cfc-8af4-4a8b-a9f2-06de5f405f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.rename(columns={'content': 'original'}, inplace=True)\n",
    "codeup_df.rename(columns={'content': 'original'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edde4c-a108-4306-ae65-13625a503473",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4924c-471f-4ed8-8d14-6b1b8d92cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f8035-be5b-4a2c-88cb-22529f75faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a df and the string name for a text column with option to pass lists for extra_words and exclude_words\n",
    "# and returns a df with the text article title, original text, stemmmed text, lemmatized text, cleaned, tokenized, & lemmatized \n",
    "# text with stopwords removed\n",
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords,\n",
    "                                  extra_words=extra_words,\n",
    "                                  exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df['clean'].apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99756d-1143-4be2-98b6-ec4c7fbc5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_article_data(news_df, 'original', extra_words = ['ha'], exclude_words = ['no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a26c05-0282-43a4-b48c-f7430e36132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_article_data(codeup_df, 'original', extra_words = ['ha'], exclude_words = ['no']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765cc68d-6b36-4d47-91e6-3b34adf6bf1a",
   "metadata": {},
   "source": [
    "### 9. Ask yourself:\n",
    "\n",
    "* If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "* If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "* If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
